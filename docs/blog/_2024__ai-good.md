说好不为技术控，我熬夜见你熬夜。
《曲折地放电》

下班了，排队打卡时
站在你旁边
”呀一一一一“

我仿佛听到
青鸟叫了一声

我羞愧地发现
你的鞋尖上，被踩了一道灰痕
向你道歉时，你眉头深深的，裙子上的皮卡丘，正曲折地放电。

问：我跟chatgpt对话，要给他喂大量数据，难道这些数据就都靠那个对话框输入输出吗？比如说我喂它一部三国演义txt文档。然后它就卡住不动了。

答：ChatGPT或者其他同类 LLM，你一次输入的内容是有限的，比如ChatGPT的GPT-4o，一次最多理论上输入输出加起来是32K的上下文窗口，英文也就是50页的样子，中文更少，所以你不可能把一本三国演义、红楼梦这样的全塞进去。

那么怎么绕过限制呢？

有几种办法：
1. 在大语言模型预训练阶段就把这些内容训练进去，比如红楼梦三国演义这种预训练中已经有了，那么就不需要再传入完整内容了。但由于预训练内容很多，所以在针对性提问时还是有可能出现幻觉或者不准确。比如你让它默写三国演义的一些特定章节不一定会很完整。

2. 将输入的内容进行微调，这样模型还是能学习一些新的知识和文本风格，但是微调时要先行对内容分块，也就是一本小说要拆成很多块，每一块可能就几页内容，并且要拆成问题和答案一对一对的，虽然是通过微调“学习”了新知识，但是并不代表它学到的知识是完整的，再加上原有知识库内容混在一起，所以针对微调的内容提问，未必会有好的效果，除非你的问题和微调时喂的问题一样的。另外微调对数据的准备要求很高，微调的成本也不低。

3. 做RAG（检索增强生成），这是目前主流的做法。简单来说就是不需要预训练你的长文本（比如三国演义），而是根据你提问的内容，去长文本里面检索出来最相关的若干段落，然后将问题和检索出来的段落，一起交给ChatGPT，让它根据问题和检索结果，回答你的问题。

比如说，你问ChatGPT：“三顾茅庐是哪三顾？”ChatGPT就根据你的问题，提炼出“三顾茅庐”关键字，找出三顾茅庐相关的几个章节，然后把这几个章节给ChatGPT：“用户问三顾茅庐是哪三顾，这里是我找出来的三顾茅庐之一顾茅庐、二顾茅庐、三顾茅庐内容如下，你来帮助回答用户的问题”，于是ChatGPT给你总结归纳了一下。这样就不需要把整一个本书都扔给它。

当然 RAG 的难点在于怎么检索到最相关的内容，这部分是相当有挑战的。
请教一下，RAG这里是不是还是主要依赖检索，比如向量数据库，通常除了开源模型，还有什么好的选择么？
RAG很依赖检索，通常是向量搜索结合关键词搜索。做RAG是不是开源模型不重要，通常好的商业模型效果更好
作为附件，让gpt查询，是查不到的。。检索能力几乎为0，而且不智能，比如给一篇小说，问一共多少个角色。。。对文档统计的能力几乎是没有的。。。
Ai一直是模拟人类在工作，人类想统计一本书，也需要花不少的时间，得做做笔记啥的吧。。甚至用数学，而且还要校对。。这一些列的工作，“模式匹配“部分占比估计也不多，所以我觉得大模型结合工程化，还是可以完成 模型本身完成不了的任务的。。。（话多了哈😂），让模型去干文档统计类的方向，就不对
如果你手搓过gpt-2，这就很容易解释了，gpt永远是一个概率预测下一个单词的机器，换句话说，它是叠加了《三国演义》，《三国志》，《莎士比亚》一系列文本之后的单一投影，至于投出来是哪一个，远不是我们这种低纬度生物可以想象的。因此，检索信息依然需要传统的mapreduce。
Role Play 核心不是 Role 而是关键词
我一直怀疑 roleplay在提示词中的作用，也说过不止一次，理由很简单，现实中我们不会这样说，训练语料中也没这东西。
它看起来能起作用，还是因为关键词触发了神经元。

只要你的后续问题，能同样包含该领域的关键词，效果应该是一样的。
问：我现在处理任何任务都想优先想提示词，提示词是越详尽越好吗，还是更加富有创造性，让ai自己在一个大框架内展开就行呢？

答：提示工程之所以是工程，一个原因就是它没有标准答案，通常需要根据你的应用场景，去不断测试、验证和改进。

所以提示词要简单还是详尽，这通常需要取决于你的应用场景，比如说你只是大致了解一篇长文内容，那么直接一句话提示词让它摘要就好了；如果你需要对长文做细致的分析并形成一个摘要性的报告给领导看，那就要多轮对话，先提炼要点，再对要点展开，再浓缩再润色。

很多情况你可能自己都不清楚该详尽还是该简单，那么原则就是从简单提示词开始，给出了反馈后看跟你期望的结果差在哪里，然后继续提后续的要求。比如说你让 AI 帮你摘要一篇长文，一句话提示词让它摘要，摘要看完后你可能有些地方不明白什么意思，就针对这些模糊的细节去提问，提了几个问题相信你就有更深入了解了，然后再让 AI 针对前面这些提问和回复一起重新摘要就是一篇更高质量的结果了。

但是要注意一点：不要太多轮会话，因为会话内容越长越容易出现幻觉，最好是在几轮会话后，就新开会话或者编辑以前的消息内容。信经过几轮会话后，对于怎么重新调整提示词你应该有了一些想法。

附原始问题：
> 宝玉老师，向您请教，最近在看了您的“如何写提示词”文档加上平时大量模仿一些质量不错的提示词后，现在我陷入了另一层问题——我现在处理任何任务都想优先想提示词，但其实有些问题可能直接表述就可以，不需要那么复杂的处理，我好像把简单问题处理的更加复杂了。我想请教，提示词是越详尽越好吗，还是更加富有创造性，让ai自己在一个大框架内展开就行呢？
目前 LobeChat 已经默认开启限制上下文限制，预设 8 条（根据我自己的长期使用经验来说，一个话题聊 3~4 轮就差不多能聊干净），并且会使用廉价模型（默认 gpt-4o-mini）自动总结历史 😋
对于大语言模型来说，它是没有记忆功能的，也就是每一次你必须发送给它所有的历史会话内容，也就是每次发新消息都会把历史消息一起发送过去。但是这样一直累加就会超出最大上下文窗口长度，并且会让会话的成本急剧上升，毕竟内容越多，需要消耗的算力越大。

所以对于  AI 聊天应用来说，在几轮会话后会自动对历史会话进行摘要，只保留最近的几次会话。这也是为什么你和 AI 聊的多了，它可能会忘记前面聊过的内容。

摘自《和 AI 对话多少轮之后重开新的会比较合适？》
我觉得问题不在于对话历史的缓存，而是在于：
1. 模型上下文窗口有长度限制
2. 每一次和模型的交互都是无状态的，也就是模型完全不会知道任何以前会话内容

这就意味着：
1. 要每次发历史会话
2. 不能把所有历史都发过去，只能选择一部分发过去
你了解一下kv cache这个东西。大模型本质是predict next token，计算好了ABCDE，把中间计算结果的kv键值缓存下来，下一次ABCDEF的时候，ABCDE部分可以直接调用缓存，只计算新增的E即可。

对大模型推理厂商来说，把之前对话的kv cache缓存下来，计算量只是增量。尤其对用户侧对话，一定有多轮上下文 ，缓存机制肯定比来回抓取压缩信息效果更好。信息压缩概括肯定还有损失
好久没分享prompt的技巧了，其实积累了好多，心累懒得发。 

很多逻辑性，结构性的知识，大语言模型经常默认提供的是bullet points模式。清晰，容易辨识。

如果下一条prompt 叫他将答案用段落写出来，比如explain the answers in narration。这时候你会得到一大段不间断的文字，以非常不同的角度。仔细读一遍，虽然主要内容一样，但是感觉完全不一样。
宝玉老师说的这个重写，我天天在用。英文rewrite，一个非常重要的词。很多时候真的就这一个词，rewrite+一大段我写的毫无章法，毫无结构的文字。让LLM搞定，万能梳理词。
我大部分情况会要求它做成table，如果结构复杂，我会让它同时按照依赖的层次关系整理一下，同样的信息用不同形式展示，确实可以方便理解。我常用table，dependency graph，mindmap，knowledge graphs
举两个 Prompt 中极致压缩的例子：
1. 苏格拉底

只要你 Prompt 设置这个角色，那么 AI 就会明白要向你提问，通过启发式的问题来引导你

2. Roast

只要你让 AI 去 roast，它就会开启吐槽模式，尤其是 Claude 更是厉害
