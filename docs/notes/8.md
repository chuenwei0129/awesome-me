一个经典的单向函数是这样的：请计算 112X1234X1239087X120385X123085324X109328403X2301934719075X293473987402389745X1290370938741098740518734018X137109284710287340182374，给出它的计算结果——这不难吧？
那好，现在咱做个反向操作：请因式分解 123098340498562094358102938410897364981261593749123784230498520498752048752347209485702938746092874568972743501874091243857。
怎么样？靠手算，全人类一起算上 50 年，可能算出来吗？
这类正向求值很容易、但逆运算却难到近乎不可能的函数，就是密码学专家绞尽脑汁寻觅的“单向函数”。
比如，我前面举的那个“乘法”和“因式分解”的例子，就是 RSA 算法所依据的基本原理。
当然，为了用于加密，它玩的要更花一些。这是因为，单纯的一个方向容易、另一个方向难到不可思议，这并不能拿来加密。想要拿来加密，就需要进一步改造这个单向函数，使得它变成另一种形式。此时，如果知道另外一个数字（也就是密钥），反向计算也会变得非常容易；但如果不知道……您就慢慢因式分解吧。满足这个特殊要求的单向函数被称为“陷门函数”。
那么，为什么“单向函数”求解 g=f(x)极其容易、但想计算 g'=f'(x)就常常要全世界的计算机一起算到宇宙毁灭呢？
个人观点——你完全可以当成是一本正经的胡说八道——是：单向函数的正向计算是一个熵增的过程；而其逆向计算却是要算出系统初态，也就是熵减。
比如，乘法，几个初值经过一定的计算，“充分混合”就得到了乘积，这个过程很容易；而因式分解呢，却要从一锅粥里面算出初始值来。
更形象一点，你买 50 斤红豆，50 斤绿豆，往一块一倒，拿搅拌机一搅，把它们搅合匀了——这很容易，对吧。现在，我要你再把这 100 斤红绿豆混合物分离开来，给我捡出 50 斤绿豆，不要见一颗红豆在里面；再检出 50 斤红豆，不要见一颗绿豆在里面——做到了，我就承认你是镇关西！哈哈，难吧？
说的概括一点：熵减比熵增难，是因为“覆水难收”。
既然微分和积分互为逆运算，为什么积分比微分更难求解？基于我仅有的大学本科高等数学知识，一般互为逆运算的操作中，分解运算比聚合运算更难。

比如：

分解操作 聚合操作

减法 加法
除法 乘法
开方 乘方
对数 指数
解方程 验证方程
解密 加密
先说锁。锁的原理和作用其他答案已经提到了，它实质上是用一个原子操作指令来保护另外的一堆非原子指令，从而使它们也得到“原子性”。所谓“原子性”，你可以理解为“做这些事时，内存只有我一个人能改，从而使得我的动作完全体现我的意图，要么完全成功要么完全失败，不会出现第三种状态”。
典型的原子操作指令如 CPU 提供的 test &amp; set 类指令：先测试内存中某个位置存储的值是否符合条件（比如为 0 表示未上锁），若符合条件则执行 set 操作（把指定值写入内存）；否则不执行任何操作。这个过程中，指令执行过程就需要禁止内存访问。不然另一个 test&set 指令就可能读到头一个 test&set 指令即将写入的单元的原始值，从而造成“脏读”。
这个过程中，指令执行过程就需要禁止内存访问。不然另一个 test&set 指令就可能读到头一个 test&set 指令即将写入的单元的原始值，从而造成“脏读”。
类似的，我们可以用 test&set 指令维护一个内存单元的内容，用它作为旗标（flag）；当我们需要读写另一块内存之前，先检查并设置旗标——当每个访问这块内存的操作都先检查和设置旗标、发现旗标状态不对就主动避让时，我们就说“这块内存被锁保护起来了，现在对它的操作都是独占的”。
显而易见，你完全可以不去请求锁（检查旗标、主动避让），那么锁对你就是完全没有强制力的。
换句话说，“锁”其实是个“君子协定”。“我”害怕这块内存脏读脏写，使“我”的程序出现 bug，所以“我”才主动调用锁，发现锁条件不满足时主动等待（除了自旋锁。锁一般是 OS 提供的，因此请求锁失败 OS 马上就会知道，就会暂时中止线程执行，直到锁被释放才会重新把它调度到待执行队列）。但如果写程序的是个野生的二蛋，他压根就不知道脏读脏写这回事……那么所谓的“锁保护”自然就不存在了（所以我喜欢把共享资源封装成个类，各种访问都必须通过接口进行）。
明白了这个，自然就该知道了：锁是（程序员自己选择的）主动退避行为，疏忽了或者学艺不精就不会知道退避，并不存在“锁什么什么位置”的说法。
或者，简单说，在这个例子里，lock/unlock 之间的代码，无论是读写内存也好、访问磁盘也罢，它们一定是串行的。绝对不存在 A lock 了、还没 unlock 呢，B 居然能抢在 A unlock 之前执行一说。写编译器的不是傻子。锁相关的指令是很特殊的（往往会带 lock 前缀、或者设置内存屏障，视不同 CPU 不同）；遇到这种指令，用脚趾头想也该知道接
程序员思维第一个，也是最显著最根源的特征是：一切程序员都是还原论者。
没错，你完全可以说，“等等！我是程序员，但还原论是啥啊？”嗯，你可以不知道什么是还原论，但你仍然是个还原论者。
所谓还原论，指的是“世界一切复杂事物都可以约分到一系列较为简单的子系统、再由简单子系统约分到很少几个简单原理上”，并且“通过很简单的几条原理、若干子系统的组合和相互作用，就可以分毫不差的精确模拟一个复杂系统”这样一种信念。持有这种信念的，就叫还原论者。
很显然，能够计算一切可计算物的图灵机就是最最典型的还原论机器。如果你身为程序员却不相信还原论、没有拆分复杂事物的能力，那么你一定会在这个行业活的非常非常的痛苦。
举例来说，熟练掌握还原论，学习知识你可以只学核心逻辑；一旦学会后，随便什么新技术什么新架构新原理，在你看来都是新瓶装旧酒：程序员基础掌握得好学新东西真的就快么？
类似的，熟练掌握还原论，那么一个外人看来非常非常麻烦的需求，在你看来就是一组核心计算法则（比如传统数据库的核心就是关系代数）的一系列组合而已。于是在别人看来得喀喀喀喀出苦力敲出无数行代码的东西，在你看来，只需设置很少几条“生成规则”，然后让计算机执行这些规则，一切功能便自然涌现。
软件工程毕竟是一个工程学科。光会拆，你永远都不可能拆出一辆整车来；同样的，做工程，你必须会组装——从最最基本的概念、标准化的各种零件/组件开始，设计一个合适的架构，把它们装配到一起，得到让客户满意的产品。
因此，任何合格的程序员，或多或少都有点系统论基础——模块如何划分？你打算传什么给我、能做到何种程度的保证？我需要回馈给你什么？如果这些都理解不了……
更进一步的，怎么通过系统各部分采集的信号，设计出合适的正负反馈机制，使得软件系统可以“自适应”于工作环境？这个东西继续发展，就是“控制论”。
但这个判断并不全面。
研究动物(包括人类)和机器内部的控制与通信的一般规律的学科，着重于研究过程中的数学关系。综合研究各类系统的控制、信息交换、反馈调节的科学，是跨及人类工程学、控制工程学、通讯工程学、计算机工程学、一般生理学、神经生理学、心理学、数学、逻辑学、社会学等众多学科的交叉学科。
在控制论中，“控制”的定义是：为了“改善”某个或某些受控对象的功能或发展，需要获得并使用信息，以这种信息为基础而选出的、于该对象上的作用，就叫作控制。由此可见，控制的基础是信息，一切信息传递都是为了控制，进而任何控制又都有赖于信息反馈来实现。信息反馈是控制论的一个极其重要的概念。通俗地说，信息反馈就是指由控制系统把信息输送出去，又把其作用结果返送回来，并对信息的再输出发生影响，起到制约的作用，以达到预定的目的。
面对复杂的系统，普通人会觉得一团乱麻：哎呀这怎么可能理得清？你胡说我胡说大家都胡说而已……
但是，很多时候，控制论可以快刀斩乱麻：再复杂，归根结底也不过是一个“冲激-响应”的问题。一个一个的把冲激输入找出来，分别讨论它的响应、分析它的因果，这不就理清楚了吗？
“控制论”已经被很多学者用到了历史研究等方面，它的确是一个极为犀利的武器。
混沌等于状态迁移特别特别多、初始条件又极端敏感；但混沌并不等于不可分析。而程序员的工作，恰恰就是找出任意复杂系统的关键点、通过几条规则破解它。这个破解的捷径，就是利用控制论方法理清系统每个局部的相互关系。
“从这堵墙翻过去，你遇到的第一个人的名字，就是你的名字。”
凡是出现了 π，那么一定有圆的存在。正因为人类看透了这一点，才会把 π 写在公式里。当然，其他答主早说透这一点了。不过我看似乎没人讲到“为什么 π 和旋转有关”……
简单说，π 不仅是圆周率，它也是“角度”。对，就和 360° 是一圈 180° 是调头一样，π 也是一样的：2π 是一圈，π 是调头，直角是 π/2。以 π 表示角度的做法叫“弧度制”。
弧度制的基本思路是，对一个半径为 1 的单位圆，以半径所夹的圆弧长度定义它对应的夹角。长度为 x 的圆弧，对应的两条半径所夹的角就是 x：
然后呢，网上就一票一票以其昏昏却欲使人昭昭者，把人搅的越来越糊涂——事实上，包括很多大学教授，他自己也是稀里糊涂……
很多人谈起“哈希表”，就会直接聚焦到 hash 函数、“散列”/“杂凑”之类不明所以的大词上，搞的初学者一头雾水，完全不明白这东西是干嘛用的，更不知道什么时候该用它、怎么用它、出现问题如何解决。
那么，这里我就从问题开始，一步步把哈希表的来龙去脉剖析清楚。想象一下：有一天，你开车到某商场去买东西。买完东西，你想不起自己的车在哪了。
这家商场非常非常大，楼下停车场少说停了上万台车；而你的健忘症又比较厉害……
那么，问题来了：你怎样才能找到自己的车？

一个理想的情形是，你从头到尾一行一行一辆一辆按顺序走一遍就找到了；运气最差时，你搜遍楼下 N 辆车，发现你的车在末尾——拿术语说，这个复杂度是 O(N)。
能不能更快一些呢？
很简单，假如所有车按车牌号顺序排列，你直接往停车场中间走就行了；如果你的车牌号大于中间那辆车，那么你就往停车场后半部分的中间走，否则就往前半场走……依此类推。如此一来，你最多只需要走 ln（N）次“中间”，就能找到你的车了。黑话叫复杂度 O(ln N)。
还能不能更给力一些？
可以。假设这个停车场非常非常大，大到可以给每个车牌号分配一个固定的停车位；那么只要你把自己的车牌号报给看门老头，他拎着你的衣领子往后一丢，你就“吧唧”一下掉自己车顶上了——嗯，你看，一车一位，就是这么任性。这就叫“查找复杂度 O(1)”。如果用程序实现的话，就是这么一个数组：car park[MAX_CAR_NUM]
第一个场景，你要直接以一个循环遍历 park 中的每个元素。第二个场景，你只需先访问 MAX_CAR_NUM 除以 2 的那个位置，再根据车牌号大小访问数组前半拉或者后半拉中间的元素即可。第三个场景，你的车牌号就是数组下标，所以你只需直接访问 park[CAR_NUMBER]即可。

那么，第三个设计是不是完全解决问题了呢？
并不是。很容易看出，第三个方案需要一个超级大的存储空间。这个空间得有多大呢？它必须大到足以和过去未来的一切有效车牌号一一对应，你才可能做到“直接按号访问”。
假设车牌号共 8 位，每位可以使用 26 个英文字母或 10 个阿拉伯数字，那么不同的车牌号共有 36^8=2821109907456 种。
哪怕每辆车只需一个字节的存储空间，这也是接近 3T 的空间！
而事实上，哪怕最大的超市，修一个够停一万辆车的停车场也都太夸张了。你看，这完全行不通啊。那么，有没有办法在得到 O(1)的查找效率的同时、又不付出太大的空间代价呢？
没错，的确是有的。这就是哈希表。
哈希表是怎么玩的呢？
很简单，我们把你的车牌号看作一个 8 位 36 进制的数字；为了方便，我们可以把它转换成十进制。那么，你的车牌号就是一个不大于 2821109907456 的数字。
现在，我们把你的车牌号除以一万，只取余数——你看，你的车牌号是不是就和 0~10000 之间的数字对应起来了？
很好，你的车就停在这个数字对应的停车位上，过去开就是了——O(1)的查找效率！
这个“把你的车牌号映射进 0~10000 之间”的操作，就是所谓的“散列”“杂凑”“哈希”或者 hash（当然，实践上，为了尽量减少冲突，哈希表的空间大小会尽量取质数）。相对于“以 key 为下标直接访问的数组”，哈希表是“时间换空间”；相对于二分法查找，哈希表又是“以空间换时间”。这种“中庸”的定位使得它在许多场合极为好用。等等，你发觉不对：我的车尾号 3456，我朋友的车也是这个尾号。我们总不能停在同一个位置吧？
你这个方案有瑕疵啊！
没错，hash 可能会把不同的数据映射到同一个点上，术语称其为“碰撞”。由于 hash 自身的基本原理，碰撞是不可避免的。
怎么解决这个“碰撞”问题呢？
几种解决思路：1、临时加个“立体车库”，哪里碰撞往哪放。于是车子就可以在同一位置“撂起来”存了。这叫“开链表法”。2、车库面积肯定是够的。3456 号被人占了，你存 3457 不就好了！换句话说，过去的散列函数是 （车牌号 模除 10000），发现碰撞了就换散列函数 （车牌号加 1 模除 10000）试一试——这叫“再散列法”。3、再修个小车库，碰撞了的停小车库去（小车库可以随便停，也可以搞一套别的机制）
总之，如此一来，我们就同时得到了“O（1）的查找效率”和“可接受的空间消耗”。
任何时候，当你有“数量有限”但“不同索引数量极大”的一些数据，必需极高的访问效率同时又不想无端消耗太多的存储空间时，你就可以考虑使用哈希表了。
当然，请注意，因为冲突的存在，哈希表虽然有着优异的平均访问时间（常数访问效率！）；但它的“最大访问时间”却是没有保证的——你可能一个微秒甚至几个纳秒就拿到了数据，也可能几十个毫秒了还在链表上狂奔。因此实时性要求严格的场合，用它前需要谨慎考虑。知道了哈希表的设计思路，我们就可以进入稍微困难一些的部分了。
我们已经知道，所谓“哈希表”，实际上是我们把对象（value）的“键值（key)”转换成了“数组下标”；然后就可以借助这个下标一步到位的找到对应对象（value）了。
但这中间有“瑕疵”存在：和身份证号和公民一一对应不同，键值和下标并不是一对一的关系。就好像你的车牌尾号是 23456 而你朋友是 53456，结果把你们安排到同一个停车位一样。
很容易想到，很多数据的键值（key）分布存在一定的规律。比如，身份证其中 4 位是你的出生年份，这四位其实只能当两位用；再如，手机号码前 3 位只可能是 135、132、153 等少数数字……那么，如果我们的“键值转换数组下标的函数（也就是哈希函数）”选择不当（比如给手机前几位、身份证的出生年份字段这些相对缺乏变化的数据过高权值），就很容易使得“碰撞”频繁出现。这就对哈希函数的选择提出了要求。
但是哈希函数本身也不应该过于复杂，不然每次计算耗时太久——O(1)虽然是常数时间；但如果时间常数太长，它可能就不如 O（lnN）查找算法快。要知道，在一百万数据里面做二分法搜索，最差时也不过需要 20 次搜索而已；如果你的哈希函数本身需要的计算时间已经超过了这个限度，那么改用二分法显然是个更为理智的选择：不仅更快，还更省空间。工程问题，向来是需要根据实际情况灵活选择、做出合理折衷的。
扯远了。
继续说哈希函数。很显然，用于哈希表的哈希函数可不能是 MD5 或者 sha1 系列函数，太慢；但也不能直接模除一个整数，太容易出现冲突。
简单说就是：哈希表用到的哈希函数，一方面要能尽量把 key 均匀散布在表空间中（从而尽量减少冲突），另一方面又要有尽量快的计算速度。
这类函数有很多种，稍微搜一搜就能找到很多。无论如何，原理所限，哈希表中碰撞无法绝对避免。当碰撞发生时，就不得不使用开链表法或再散列法存储冲突数据；而这必将影响哈希表的性能。
很容易想到，如果哈希表很大、里面却没存几条数据，那么它出现冲突（碰撞）的几率就会很小；反之，如果哈希表已经接近满了，那么每条新加入的数据都会产生碰撞。换句话说，在哈希函数选择合理的前提下，想要减少碰撞，就只能扩大哈希表占用的空间。
哈希表实际所存数据量和哈希表最大容量之间的比值，叫做哈希表的“加载因子”。加载因子越小，冲突的概率就越低，但浪费大量空间；加载因子越高，冲突概率越大，但空间浪费就越少。这是一个需要根据工程实践灵活选择的折衷值。很多语言提供的 hash 表允许你主动调节这个值。一般来说，一个较为平衡的加载因子大约是 0.7~0.8 左右。这样既不会浪费太多空间，也不至于出现太多冲突。
另一方面，因为哈希表使用的哈希函数较为简单，因此对恶意的攻击者来说，他可以精心构造一大堆数据提交给你——所有这些数据散列后全都存在一个格子里。我们前面提到过，当遇到这种冲突/碰撞时，为了避免彼此覆盖，这些数据就要存在链表中（或者再散列后存在同一个哈希表中）。当这些数据被存进链表时，对它们的访问效率将降到 O(N)——因为链表搜索效率只有 O(N)。之前就发生过这种攻击，包括 Java 在内的许多种语言全部落马。
解决方案也很简单：1、提高哈希函数复杂度，想办法加入随机性（相当于每次使用一个不同的哈希函数），避免被人轻易捕捉到弱点 2、不要用开链表法存储冲突数据，采用“再散列法”，并且使用不同的哈希函数再散列、还可以把冲突数据存入另一个表——要构造同时让两个以上不同的哈希函数冲突的攻击数据，难度就大得多了。总之，哈希表是用途广泛的一种数据结构，也是很多编程语言提供的基础服务之一（比如 python 的 dict）。
你完全可以傻瓜式的使用它，无须搞懂它的一切；但想要把它用精、用好，你还是需要真正理解它的来龙去脉——千万不要 像某种最好的语言的作者那样，拿函数名长度当哈希值（哎呀一不小心又黑人了，顶锅逃……
稍微展开说两句。
“内存泄漏”为什么叫“泄漏”？叫“不释放”行不行？答案是：不行。内存泄漏的诱因的确是分配了内存然后没有释放。真找到原因了，也的确让人恨不得指着肇事者的鼻子骂“你怎么不释放”——但，“不释放”和“泄漏”并不等价。
关于“内存泄漏”更形象的比喻，就是汽车、自行车轮胎漏气：
看起来完整、硬实？用不了几个小时车轮就瘪了。你直接看，到处找，怎么都找不到漏气点。怎么办呢？弄个大盆子，装上水，把轮胎一段段泡里面，看哪里有气泡冒出：
最初，人们并不知道“面向对象”究竟应该是什么、“继承”又该占据什么位置——对一种新生事物，要求人们一下子就在头脑里有个清晰图景显然是不可能的。
关于面向对象，一直以来就有两个主要派别：Class-based vs prototype-based 后来的其他各种流派，都离不开这两个派别的核心思路，只是具体细节上略有不同而已。
其中，前者认为，面向对象就是个分类问题；既然是分类问题，那么根据生活经验，更靠“上”更“抽象”的大类自然就更基础，它所有的东西理所当然应该被继续细化的“子”类“继承”——圆形是个图形，方形也是个图形，所以圆形和方形都应该从“图形”这个类继承。类似的，蝙蝠既是可以飞行的动物，也是哺乳动物，所以它就应该从“可飞行动物类”和“哺乳动物类”继承——这样才可以“既能飞行又能哺乳”。
换句话说，Class-based 这个思路很容易直接导向一个误区，那就是不假思索的引入“继承”，并且还总是把“继承”看得过重。——但是，如此一来，就不可避免的导致很多含糊不清的问题。其中表现最严重的就是多重继承。比如，蝙蝠究竟是用飞行动物的嘴吃饭呢，还是用哺乳动物的嘴吃饭？吃下的饭，是给哺乳动物的胃消化呢，还是给飞行动物的胃消化？（熟悉编程的朋友恐怕马上就要想到未初始化、未重置、访问错误的内存区域等等“恶心而又可怕”的东西了）
有的人可能不假思索的说“没关系没关系，C++的虚继承了解下！”：似乎只要在语言中提供个机制，把来自飞行动物和来自哺乳动物的嘴巴、胃等等合并起来就够了。你看，现在不存在歧义了吧？——嗯……我现在有个需求，我们知道，汽车过去都是内燃机车，后来有了电动车；但是电动车充电慢电池容量小，所以又有了混动车。请问，当我的混动车同时从内燃机车和电动车多重继承后，你会不会自作主张把两个不同的动力基类合并？你要合并了，我这程序还怎么写？我的车上的的确确有两个不同的发动机！但倘若你不合并……你看，菱形继承的二义性就又来了。
当然，“多重继承”只是最明显、最恶劣最不可调和的矛盾之一。“继承”带来的很多其他问题，诸如“正方形是不是长方形”之类，迫使 Class-based 流派不得不重新思考自己的根基，并最终将语言中的“类”和日常语言所说的类彻底区分开来，这就是所谓的“is-a”。——但是，一旦类不再是类而是“is-a”，那么我们就着日常用语习惯总结出来的各种设计方法论，还能打捞出多少有用的东西呢？在这种用语习惯带来的误导干扰下，这东西真的能利大于弊吗？
prototype-based 派别认为，面向对象其实就是一组实现了特定协议（或者叫接口）的 object——在它里面压根就不存在类，只有 prototype 和 object（最初的 prototype-based 相关理论远没有如今成熟。像这样一句话说清自己的本质，是经过长久的发展、争论后才能办到的：基础思路虽好，但却说不清楚，这就使得它和“立竿见影的得到很多很多好处”的 class-based 派的竞争中处于不利地位）。
按照这一派的思路发展下去，我们真正应该关心的是“对象可以提供什么样的服务（或者说，像 XXX 一样的服务）”：重要的是接口！压根就不需要考虑/支持继承这种矫揉造作的东西！
这就绕开了 class-based 需要面对的、棘手的“正方形是不是一种长方形”问题——程序语言里面的 class 并不是日常语言中的“类”，它的精确表述是“is-a”，和口语的“类”八竿子打不着（事实上，自从 class-based 派同意“类不是类而是 is-a”开始，他们已经向 prototype 派投降了：你可以自己想想这是为什么）。
事实上，几乎从最初的几个版本开始，C++/Java 就引入了 prototype 流派的思想，这就是所谓的“interface”，或者说，其实严格来说并不是继承的“接口继承”——当然，基于一贯的、对程序员的信任，C++允许你的 interface 里面存在实现代码甚至数据成员：只要你确切知道它会被如何使用。这种做法就使得接口继承里面的继承二字又找回了一定的存在感，然后就把多重继承之类问题又找回来了。
为什么 prototype-based 派可以绕开继承带来的诸多副作用呢？很简单，因为 prototype 派压根就不存在继承。
它就是声明自己支持某个“协议/接口/prototype（反正就这意思，你叫它什么都可以）”，然后想办法真的去支持这个协议就完了。
既然 prototype 只是允许一个对象声明它兼容某个 prototype 而已，并不会越俎代庖的把这个 prototype 的默认实现/标准基类等等东西塞进你的代码——那么，这个 prototype 究竟是怎么搞出来的，当然就由你完全控制了：哪怕你往里面塞一万个同样支持这个 prototype 的 object 进去，只要你自己头脑清醒、知道什么时候应该把调用转给这一万个 object 中的哪一个，它就是完全合法并且井井有条的。
网线的确称得上“历史悠久”，但绝对和“古董”扯不上关系。、事实上，为了让它能够把信号送到 100 米开外（7 类以下）或者 30 米外（8 类线），网线借助了艰深的“传输线”理论。
简单说，铜线上传输高频电信号时，是会有电磁波发射的，这本身就会使得信号快速衰减；雪上加霜的是，发射出的电磁波还会被导线自身乃至另一根导线吸收、产生感应电势——懂电学的都知道，这就是“信号串扰”，会大幅降低对端接收到信号的信噪比。尤其是，双工通讯时，网线中本来就有两个信号，两个信号相互串扰同时还干扰自身，情况就更复杂了——所以你传音频这种低频模拟信号，只需随便拉根屏蔽线，传几百米甚至几公里都没问题；但传输动辄几十几百 M 甚至若干 G 的高频信号，你要拿音频线来……哪怕拿发烧友用的纯银编织线都无济于事，信号至多传个三五米就会衰减殆尽。
为了应对这种情况，技术人员通过计算，巧妙的把网线线缆中的信号线两两对绞——也就是俗话说的“拧”——在一起。这种“对绞”使得信号线彼此缠绕，产生类似于“变压器”的效果，使得导线上辐射出的电磁信号马上被自身吸收，就好像变压器主线圈上的信号能量被转移到了次级线圈一样——这就是所谓的“传输线变压器”
举例来说，Android 有个 webview 组件。这个东西在 Android 4.4 之前基于 webkit，之后就是 chrome 内核了——这玩意儿对 H5 的支持还是很不错的。
但是，国内手机厂商不约而同的阉割了这个组件，故意制造不兼容。结果就是，除非你的 app 自带浏览器内核，否则就别想正常运行。
为什么我说他们是故意呢？因为最近调一个程序，丢浏览器能跑，放手机上不行；再一试手机自带浏览器，也不行——然而 Google play 装的 chrome 又行了。于是决定看看究竟哪的问题，打开http://html5test.com，手机自带浏览器得分还挺高；尤其其中我们程序功能相关的特性居然也有。那怎么不执行我们的程序呢？想了想，我尝试重新打开过去跑不过的那个应用。顺利打开。明白了，发现你跑html5test，它就临时打开开关，让你看它特性支持的足足的；不跑，那就不给你支持。
于是继续查，看看 webview 能跑多少分——果然，webview 得分极低，把本来支持的近 80 项 html5 特性都关掉了。这就使得这些阉割版系统的 webview 组件压根没法用。除非自带浏览器内核，绕开这个玩意儿。
举例来说，国内某龙头企业的即时通讯软件，它的国内版就不得不带了个蹩脚的浏览器组件。不然很多功能就没法实现。但这个组件使得软件变得更加臃肿；且优化水平很差，使得这玩意儿用起来总是能明显感觉到卡卡的——正常的 js 引擎跑 Javascript，如果还能有 C 的性能 1/20 的话，这个组件可能就只能到 1/23 或者 1/25，甚至更差。
如果你用的是海外版或者国外品牌的手机，那么可以通过控制台卸载这个组件，让它用标准的 webview，流畅度就会立竿见影的提升；但如果你用的是国内版的手机……卸载这个组件只会让它崩溃、或者页面故障。类似的，你直接从 Google play 装的这个软件，它就原生不带这个组件——后果嘛，那就是流畅多了。当然，前提是你不能用国产阉割版手机系统。卡点总比用不了强。
类似的，知乎手机版也自带了一个很烂的浏览器。chromium 87 内核，html5test 只有 469 分。但这个玩意儿比起手机自带的破烂油腻 UI 浏览器的 444 分还是高了不少。可想而知，如果不浪费你的空间、使用阉割更严重的手机自带 webview 的话，体验会烂到什么地步、开发测试又会不可控到什么地步。
你看，为什么类似压线钳或者第一个虚拟机项目的场景里，“耦合”反而更简洁、更方便；但越到后来，耦合越是成了背不动的包袱？
很简单：压线钳/装机间是高度特化的工具。它本来就和制作网线、配置虚拟机是一体的。我们为这种“耦合”起了个专门的名字，叫“内聚”。
内聚的意思就是，本来就是一体的东西，你别拆出来。把它留在壳子里就对了。你不把它留在壳子里，而是提溜出来，放到商城项目里去做，就使得“商城”这个项目和虚拟机这个项目严重耦合了。
没错。工作间是一个很漂亮的抽象；但它只应该出现在虚拟机项目里，不应该让商城知道。
依赖倒置是什么？
依赖倒置的意思是，不要一见到需求，就马上着急思考“这个项目该怎么完成”“第一步做什么第二步做什么”……而是要先站在更高的层面上，总览整个需求，把不同的模块斩开。斩开之后，不要着急定接口，更不要着急思考模块内部的实现细节；而是要先思考模块之间的关系。
举例来说，我前面的框图里，给出了一个“虚拟机商品化模块”；然后我们知道，更上一层有个商城模块——那么现在，让我们思考一下，“虚拟机商品化模块”和“商城模块”之间，应该是什么关系、它们应该以什么为中心。
这就是我前面提到的，在第一次参与这个会议时，我就明确提出的——你们先别扯淡了，咱先确定一件事，那就是：什么是商品！
依赖倒置原则（DependenceInversionPrinciple，DIP）是指设计代码结构时，高层模块不应该依赖低层模块，二者都应该依赖其抽象。
